provider "google" {
  credentials = file("KEYFILE")
  project = "PROJECT"
  region  = "REGION"
  zone    = "ZONE"
}

resource "google_dataproc_cluster" "CLUSTER" {
  name   = "CLUSTER"
  region = "REGION"

  provisioner "local-exec" {
    command = "gcloud auth activate-service-account --key-file KEYFILE; gsutil mb -p PROJECT gs://PROJECT-jars; gsutil cp JAR_PATH gs://PROJECT-jars/JAR_NAME"
  }
}

resource "google_dataproc_job" "spark" {
  region       = google_dataproc_cluster.CLUSTER.region
  force_delete = true
  placement {
    cluster_name = google_dataproc_cluster.CLUSTER.name
  }

  spark_config {
    main_class    = "com.example.Pi"
    jar_file_uris = ["gs://PROJECT-jars/JAR_NAME"]
    args          = ["1000"]

    properties = {
      "spark.logConf" = "true"
    }

    logging_config {
      driver_log_levels = {
        "root" = "INFO"
      }
    }
  }
}

output "spark_status" {
  value = google_dataproc_job.spark.status[0].state
}
